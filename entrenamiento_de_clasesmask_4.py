# -*- coding: utf-8 -*-
"""Entrenamiento de ClasesMASK_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JXeeJu2rDsOo2ll3lxK7T85Ou7ThKyme
"""

!nvidia-smi

"""# **VERIFICAR ENTORNO**"""

import cython
import h5py
import imgaug
import ipython_genutils
import keras
import matplotlib
import numpy
import cv2
import skimage
import scipy
import tensorboard
import tensorflow

print("Versión de Cython:", cython.__version__)
print("Versión de h5py:", h5py.__version__)
print("Versión de imgaug:", imgaug.__version__)
print("Versión de ipython-genutils:", ipython_genutils.__version__)
print("Versión de Keras:", keras.__version__)
print("Versión de Matplotlib:", matplotlib.__version__)
print("Versión de NumPy:", numpy.__version__)
print("Versión de OpenCV (opencv-contrib-python):", cv2.__version__)
print("Versión de scikit-image:", skimage.__version__)
print("Versión de SciPy:", scipy.__version__)
print("Versión de TensorBoard:", tensorboard.__version__)
print("Versión de TensorFlow:", tensorflow.__version__)

import cv2
import os
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

print("OpenCV version:", cv2.__version__)
print("PIL version:", Image.__version__)
print("Numpy version:", np.__version__)

#https://github.com/akTwelve/Mask_RCNN

"""# **CARGAR LIBRERIAS**"""

import os
import sys
import json
import numpy as np
import time
from PIL import Image, ImageDraw
import skimage.draw
import random
from google.colab import drive
# Montar Google Drive
#drive.mount('/content/drive')

"""# **CLONAR REPOSITORIO GITHUB**"""

# Clone the repo
#!git clone https://github.com/andryshalom/MASK_RCNN_TF2_14_EDIT.git maskrcnn
!git clone https://github.com/andryshalom/DECAFIA-MASK_RCNN_TF2_14_EDIT.git maskrcnn
# Change the runtime directory to the cloned repo
import os
os.chdir('/content/maskrcnn/')

# Download pre-trained weights
!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5

"""# **PROBAR MODELO DE COCO (GPU)**"""

import mrcnn
import mrcnn.config
import mrcnn.model
import mrcnn.visualize
import cv2
import os

# load the class label names from disk, one label per line
# CLASS_NAMES = open("coco_labels.txt").read().strip().split("\n")

CLASS_NAMES = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']

class SimpleConfig(mrcnn.config.Config):
    # Give the configuration a recognizable name
    NAME = "coco_inference"

    # set the number of GPUs to use along with the number of images per GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

    # Number of classes = number of classes + 1 (+1 for the background). The background class is named BG
    NUM_CLASSES = len(CLASS_NAMES)

# Initialize the Mask R-CNN model for inference and then load the weights.
# This step builds the Keras model architecture.
model = mrcnn.model.MaskRCNN(mode="inference",
                             config=SimpleConfig(),
                             model_dir=os.getcwd())

# Load the weights into the model.
# Download the mask_rcnn_coco.h5 file from this link: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
model.load_weights(filepath="mask_rcnn_coco.h5",
                   by_name=True)

# load the input image, convert it from BGR to RGB channel
image = cv2.imread("test.jpg")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Perform a forward pass of the network to obtain the results
r = model.detect([image], verbose=0)

# Get the results for the first image.
r = r[0]

# Visualize the detected objects.
mrcnn.visualize.display_instances(image=image,
                                  boxes=r['rois'],
                                  masks=r['masks'],
                                  class_ids=r['class_ids'],
                                  class_names=CLASS_NAMES,
                                  scores=r['scores'])

"""#**PROBAR COCO (CPU) solo para CPU**"""

import mrcnn
import mrcnn.config
import mrcnn.model
import mrcnn.visualize
import cv2
import os
import tensorflow as tf  # Importar TensorFlow
from PIL import Image
import matplotlib.pyplot as plt

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # Configurar para usar solo CPU

# load the class label names from disk, one label per line
# CLASS_NAMES = open("coco_labels.txt").read().strip().split("\n")

CLASS_NAMES = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']

class SimpleConfig(mrcnn.config.Config):
    # Give the configuration a recognizable name
    NAME = "coco_inference"

    # set the number of GPUs to use along with the number of images per GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

    # Number of classes = number of classes + 1 (+1 for the background). The background class is named BG
    NUM_CLASSES = len(CLASS_NAMES)

# Initialize the Mask R-CNN model for inference and then load the weights.
# This step builds the Keras model architecture.
with tf.device('/CPU:0'):  # Force Tensorflow to use CPU
    model = mrcnn.model.MaskRCNN(mode="inference",
                                 config=SimpleConfig(),
                                 model_dir=os.getcwd())

    # Load the weights into the model.
    # Download the mask_rcnn_coco.h5 file from this link: https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
    model.load_weights(filepath="mask_rcnn_coco.h5",
                       by_name=True)

    # load the input image, convert it from BGR to RGB channel
    image = cv2.imread("test.jpg")
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Perform a forward pass of the network to obtain the results
    r = model.detect([image], verbose=0)

    # Get the results for the first image.
    r = r[0]

    # Visualize the detected objects.
    #result_image=mrcnn.visualize.display_instances(image=image,
    #                                  boxes=r['rois'],
    #                                  masks=r['masks'],
    #                                  class_ids=r['class_ids'],
    #                                  class_names=CLASS_NAMES,
    #                                  scores=r['scores'])

    result_image = mrcnn.visualize.display_instances2(image=image,
                                  boxes=r['rois'],
                                  masks=r['masks'],
                                  class_ids=r['class_ids'],
                                  class_names=CLASS_NAMES,
                                  scores=r['scores'])

    # Directorio donde quieres guardar la imagen
    output_dir = "/content/maskrcnn/images"
    os.makedirs(output_dir, exist_ok=True)  # Crear el directorio si no existe

    # Nombre del archivo de la imagen
    file_name = "result_image.jpg"

    # Ruta completa del archivo
    file_path = os.path.join(output_dir, file_name)

    # Guardar la imagen en disco
    with open(file_path, "wb") as f:
        f.write(result_image.getvalue())

    print("La imagen ha sido guardada en:", file_path)

import matplotlib.pyplot as plt
import cv2
# Cargar la imagen utilizando OpenCV
image = cv2.imread("/content/maskrcnn/images/result_image.jpg")

# Convertir la imagen de BGR a RGB (si es necesario)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Mostrar la imagen utilizando Matplotlib
plt.imshow(image_rgb)
plt.axis('off')
plt.show()

import io
import matplotlib.pyplot as plt
import numpy as np

def visualize_image(image):
    plt.imshow(image.astype(np.uint8))
    plt.axis('off')
    plt.tight_layout()

    # Guardar la imagen en un buffer
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)
    plt.close()

    # Retornar el contenido del buffer
    return buf.getvalue()

# Ejemplo de uso:
result_image_bytes = visualize_image(result_image)

import matplotlib.pyplot as plt

# Mostrar la imagen resultante
plt.imshow(result_image)
plt.axis('off')  # Desactivar los ejes
plt.show()

#!git clone https://github.com/DavidReveloLuna/MaskRCNN_Video.git



"""# CONFIGURACION MASK"""

cd maskrcnn/

#!python setup.py install

ROOT_DIR = '/content/maskrcnn'
assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist'

sys.path.append(ROOT_DIR)

from mrcnn import visualize
from mrcnn.config import Config
from mrcnn import model as modellib, utils

# Directory to save logs and trained model
MODEL_DIR = os.path.join(ROOT_DIR, "logs")

# Local path to trained weights file
COCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")

# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_MODEL_PATH):
    utils.download_trained_weights(COCO_MODEL_PATH)

class CustomConfig(Config):
    """Configuration for training on the helmet  dataset.
    """
    # Give the configuration a recognizable name
    NAME = "object"

    # Número de imágenes a entrenar en cada GPU. Una GPU de 12 GB puede manejar típicamente
    # 2 imágenes de 1024x1024 px.
    # Ajusta según la memoria de tu GPU y el tamaño de las imágenes. Utiliza el número más alto
    # que tu GPU pueda manejar para un mejor rendimiento
    # BATCH_SIZE=IMAGES_PER_GPU * GPU_COUNT
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

    # Number of classes (including background)
    NUM_CLASSES = 1 + 4  # background + objetos

    # All of our training images are 960x960
    # la dimencion maxima debe ser multiplo de 64
    IMAGE_MIN_DIM = 192
    IMAGE_MAX_DIM = 960

    # BATCH_SIZE=1    # tamaño del BACH (division del conjunto de datos train)
    # BATCH_SIZE=IMAGES_PER_GPU * GPU_COUNT
    # STEPS_PER_EPOCH = num_de_imagenes_train/batch_size (se recorre almenos una vez cada imagen redondeo hacia la derecha)
    # STEPS_PER_EPOCH = 813/BATCH_SIZE
    # Número de pasos de entrenamiento por época.
    # Esto no necesita coincidir con el tamaño del conjunto de entrenamiento. Los registros de Tensorboard
    # se guardan al final de cada época, por lo que establecer esto en un número
    # más pequeño significa obtener actualizaciones más frecuentes de TensorBoard.
    # Las estadísticas de validación también se calculan al final de cada época y pueden llevar un tiempo,
    # así que no lo establezcas muy pequeño para evitar pasar mucho tiempo en estadísticas de validación.
    STEPS_PER_EPOCH =   2130 #1989 #1951 #1934

    # Número de pasos de validación que se ejecutarán al final de cada época de entrenamiento.
    # Un número mayor mejora la precisión de las estadísticas de validación, pero ralentiza el entrenamiento.
    # no hacer mas de 10
    VALIDATION_STEPS = 25

    # Arquitectura de la red "backbone".
    # Los valores soportados son: resnet50, resnet101.
    # También puedes proporcionar una función que debe tener la firma
    # de model.resnet_graph. Si lo haces, también necesitas proporcionar una función
    # a COMPUTE_BACKBONE_SHAPE.

    BACKBONE = 'resnet101' #'resnet50'

    # Media RGB para nuestra base de datos: [ 91.78358868 129.7007309  112.42858215]
    MEAN_PIXEL = np.array([82.8, 124.3, 99.7]) # Media RGB: [ 82.7938383  124.32991352  99.68951586]

    # imagenes de 640x640  (Escalas de Anclaje para la RPN):
    # es recomendable ajustar las escalas de anclaje (RPN_ANCHOR_SCALES) para abarcar adecuadamente este rango de tamaños
    # La elección de las escalas de anclaje puede depender de las características específicas de tu conjunto de datos.
    # Experimenta con diferentes configuraciones y observa cómo afectan la detección de objetos en tu escenario particular.
    # Longitud del lado de la ancla cuadrada en píxeles
    RPN_ANCHOR_SCALES = (24,  38,  80, 480, 880)  # [ 19.  37. 114. 395. 789.] #(16,  32,  64, 176, 365, 602, 860)   # #[ 19.  37. 114. 408. 777.](17, 33, 101, 408, 771)   #(27, 75, 192, 450, 810) #(18, 50, 128, 300, 540)*1.5
    #(18, 50, 128, 300, 540) para la base de datos de 640 pero como se aumento un 50% se multiplica 1.5
    RPN_ANCHOR_RATIOS = [0.75, 1.0, 2.25]
    # The strides of each layer of the FPN Pyramid. These values
    # are based on a Resnet101 backbone.
    BACKBONE_STRIDES = [4, 8, 16, 32, 64]#   [2, 4, 8, 16, 32, 64, 128]


    # Tasa de aprendizaje y momentum
    # El documento Mask RCNN utiliza lr=0.02, pero en TensorFlow provoca
    # que los pesos exploten. Probablemente debido a diferencias en la implementación del optimizador.
    LEARNING_RATE = 0.001 #0.0008 #0.001
    LEARNING_MOMENTUM = 0.9

    # Regularización de peso
    #prevenir el sobreajuste (overfitting) y mejorar la generalización del modelo.
    #La idea básica detrás de la regularización de peso es
    # penalizar los pesos grandes en la función de pérdida durante el entrenamiento.
    WEIGHT_DECAY = 0.0005#0.0001



    # Umbral de supresión no máxima para filtrar propuestas RPN.
    # Puedes aumentar esto durante el entrenamiento para generar más propuestas.
    RPN_NMS_THRESHOLD = 0.75 #0.7

    # Valor de probabilidad mínimo para aceptar una instancia detectada
    # ROIs por debajo de este umbral se omiten
    DETECTION_MIN_CONFIDENCE = 0.79 #0.8 #0.74 # 0.7

    # Umbral de supresión no máxima para detección
    DETECTION_NMS_THRESHOLD = 0.5

    # Si está habilitado, redimensiona las máscaras de instancia a un tamaño más pequeño para reducir
    # la carga de memoria. Recomendado cuando se usan imágenes de alta resolución.
    USE_MINI_MASK = False
    MINI_MASK_SHAPE = (60, 60)  # (altura, ancho) de la mini-máscara

    # Número de ROIs por imagen para alimentar a las cabezas clasificadoras/máscara
    # El documento Mask RCNN utiliza 512, pero a menudo el RPN no genera
    # suficientes propuestas positivas para llenar esto y mantener una relación positiva:negativa
    # de 1:3. Puedes aumentar el número de propuestas ajustando
    # el umbral de RPN NMS.

    # Especifica el número de Regiones de Interés (ROIs) que se seleccionan para el entrenamiento de cada imagen.
    # Las ROIs son regiones propuestas que se utilizan para entrenar la red en la tarea de clasificación
    # y regresión de cajas delimitadoras.
    TRAIN_ROIS_PER_IMAGE = 200


    #Limita el número máximo de instancias (objetos) anotadas que se utilizarán durante el entrenamiento.
    # Si tienes un conjunto de datos con un gran número de instancias, este parámetro puede ayudar a controlar
    # la cantidad de datos utilizados durante cada iteración de entrenamiento.
    MAX_GT_INSTANCES = 78

    # (ROIs después de la supresión no máxima - Inferencia):
    #bEspecifica el número de ROIs que se conservarán después de aplicar la supresión no máxima durante la inferencia.
    # Después de la predicción de la RPN, se utilizan técnicas de supresión no máxima para reducir el número de
    # propuestas redundantes.
    POST_NMS_ROIS_INFERENCE = 2000

    #(ROIs después de la supresión no máxima - Entrenamiento):

    # Similar a POST_NMS_ROIS_INFERENCE, pero aplicado durante el entrenamiento. Después de la predicción de la RPN
    # y antes de pasar a las etapas subsiguientes del modelo, se aplica la supresión no máxima para limitar
    # el número de ROIs utilizadas durante el entrenamiento.
    POST_NMS_ROIS_TRAINING = 1000
    #
    # Número máximo de detecciones finales
    DETECTION_MAX_INSTANCES = 78

    # Porcentaje de ROIs positivas utilizadas para entrenar cabezas clasificadoras/máscara
    ROI_POSITIVE_RATIO = 0.3 #0.33

    # Esto significa que los parámetros de escala y desplazamiento de la BN se optimizarán
    # junto con los demás parámetros de la red durante el entrenamiento.
    TRAIN_BN = True

    # Pesos de pérdida para una optimización más precisa.
    # Pueden usarse para la configuración de entrenamiento R-CNN.
    # controla el peso de la insidencia de una funcion de costo especifica
    LOSS_WEIGHTS = {
        "rpn_class_loss": 1.,
        "rpn_bbox_loss": 1.,
        "mrcnn_class_loss": 1.,
        "mrcnn_bbox_loss": 1.,
        "mrcnn_mask_loss": 1.
    }

config = CustomConfig()
config.display()

#from google.colab import drive
# Montar Google Drive
#drive.mount('/content/drive')

"""# COMBINAR DOS CARPETAS PARA HACER UNA SOLA RUTA DE ENTRENAMIENTAMIENTO"""

!pip install rarfile
import shutil
import rarfile
import json
import os

def descomprimir_rar(ruta_rar):
    # Crear una instancia de la clase RarFile
    with rarfile.RarFile(ruta_rar) as rf:
        # Extraer todos los archivos en la misma carpeta que el .rar
        rf.extractall(os.path.dirname(ruta_rar))
# Definir las rutas de las carpetas originales
rutas_multiples = ["/content/maskrcnn/images/train4", "/content/maskrcnn/images/train4b","/content/maskrcnn/images/train4c"]

# Definir la ruta de la nueva carpeta
ruta_nueva = "/content/maskrcnn/images/train_merged"

# Crear la carpeta nueva si no existe
if not os.path.exists(ruta_nueva):
    os.makedirs(ruta_nueva)

# Mover el contenido de las carpetas originales a la carpeta nueva
for ruta in rutas_multiples:
    # Obtener la lista de archivos en la carpeta original
    archivos = os.listdir(ruta)
    # Mover cada archivo a la carpeta nueva
    for archivo in archivos:
        shutil.move(os.path.join(ruta, archivo), ruta_nueva)

print("¡Contenido de las carpetas movido exitosamente a la nueva carpeta!")

"""# DESCOMPRIMIR LAS ETIQUETAS"""

# Ruta del archivo .rar en Google Drive
ruta_rar = "/content/maskrcnn/images/val4/via_region_data.rar"
# Descomprimir el archivo .rar
descomprimir_rar(ruta_rar)
# Ruta del archivo .rar en Google Drive
ruta_rar = "/content/maskrcnn/images/train_merged/via_region_data.rar"
# Descomprimir el archivo .rar
descomprimir_rar(ruta_rar)

"""# CUSTOMDATSET"""

class CustomDataset(utils.Dataset):

    def load_custom(self, dataset_dir, subset):
        """Load a subset of the bottle dataset.
        dataset_dir: Root directory of the dataset.
        subset: Subset to load: train or val
        """
        # Add classes. We have only one class to add.
        self.add_class("object", 1, "HOJAS")
        self.add_class("object", 2, "ROYA")
        self.add_class("object", 3, "COCO")
        self.add_class("object", 4, "MINADOR")
        #self.add_class("object", 5, "SANAS")

        # Train or validation dataset?
        assert subset in ["train_merged", "val4"] #["TRAIN960", "VAL960"]
        dataset_dir = os.path.join(dataset_dir, subset)

        # Load annotations
        # VGG Image Annotator saves each image in the form:
        # { 'filename': '28503151_5b5b7ec140_b.jpg',
        #   'regions': {
        #       '0': {
        #           'region_attributes': {},
        #           'shape_attributes': {
        #               'all_points_x': [...],
        #               'all_points_y': [...],
        #               'name': 'polygon'}},
        #       ... more regions ...
        #   },
        #   'size': 100202
        # }
        # We mostly care about the x and y coordinates of each region
        annotations1 = json.load(open(os.path.join(dataset_dir, "via_region_data.json"))) ## se puede cambiar el nombre
        # print(annotations1)
        annotations = list(annotations1.values())  # don't need the dict keys

        # The VIA tool saves images in the JSON even if they don't have any
        # annotations. Skip unannotated images.
        annotations = [a for a in annotations if a['regions']]

        # Add images
        for a in annotations:

            if isinstance(a['regions'], list):
                polygons = [r['shape_attributes'] for r in a['regions']]
                objects = [s['region_attributes']['objetos'] for s in a['regions']]
            elif isinstance(a['regions'], dict):
                polygons = [r['shape_attributes'] for r in a['regions'].values()]
                objects = [s['region_attributes']['objetos'] for s in a['regions'].values()]
            else:
                # Manejar otros casos si es necesario
                print("a['regions'] no es ni una lista ni un diccionario conocido")

            name_dict = {"HOJAS": 1,"ROYA": 2,"COCO": 3,"MINADOR": 4}  #{"ENFERMAS": 1,"ROYA": 2,"COCO": 3,"MINADOR": 4,"SANAS": 5}
            num_ids = [name_dict[a] for a in objects]

            nombree=os.path.splitext(os.path.basename(a['filename']))[0]+".jpg"#f"{os.path.basename(a['filename'])}.jpg"
            image_path = os.path.join(dataset_dir, nombree)
            image = skimage.io.imread(image_path)
            height, width = image.shape[:2]

            self.add_image(
                "object",  ## for a single class just add the name here
                image_id=nombree,  # a['filename']use file name as a unique image id
                path=image_path,
                width=width, height=height,
                polygons=polygons,
                num_ids=num_ids)

    def load_mask(self, image_id):
        """Generate instance masks for an image.
       Returns:
        masks: A bool array of shape [height, width, instance count] with
            one mask per instance.
        class_ids: a 1D array of class IDs of the instance masks.
        """
        # If not a bottle dataset image, delegate to parent class.
        image_info = self.image_info[image_id]
        if image_info["source"] != "object":
            return super(self.__class__, self).load_mask(image_id)

        # Convert polygons to a bitmap mask of shape
        # [height, width, instance_count]
        info = self.image_info[image_id]
        if info["source"] != "object":
            return super(self.__class__, self).load_mask(image_id)
        num_ids = info['num_ids']
        mask = np.zeros([info["height"], info["width"], len(info["polygons"])],
                        dtype=np.uint8)
        for i, p in enumerate(info["polygons"]):
            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])
            rr = np.clip(rr, 0, info["height"] - 1)
            cc = np.clip(cc, 0, info["width"] - 1)
            #print("Coordinates before:", rr, cc)
            mask[rr, cc, i] = 1

        # Return mask, and array of class IDs of each instance. Since we have
        # one class ID only, we return an array of 1s
        # Map class names to class IDs.
        num_ids = np.array(num_ids, dtype=np.int32)
        return mask, num_ids

    def load_mask2(self, image_id):
        """Generate instance masks for an image.
        Returns:
        masks: A bool array of shape [height, width, instance count] with
            one mask per instance.
        class_ids: a 1D array of class IDs of the instance masks.
        """
        # If not a bottle dataset image, delegate to parent class.
        image_info = self.image_info[image_id]
        if image_info["source"] != "object":
            return super(self.__class__, self).load_mask(image_id)

        # Convert polygons to a bitmap mask of shape
        # [height, width, instance_count]
        info = self.image_info[image_id]
        if info["source"] != "object":
            return super(self.__class__, self).load_mask(image_id)
        num_ids = info['num_ids']
        mask = np.zeros([info["height"], info["width"], len(info["polygons"])],
                        dtype=np.uint8)
        bounding_boxes = []
        for i, p in enumerate(info["polygons"]):
            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])
            rr = np.clip(rr, 0, info["height"] - 1)
            cc = np.clip(cc, 0, info["width"] - 1)
            mask[rr, cc, i] = 1

            # Find bounding box coordinates
            y_min, y_max = np.min(rr), np.max(rr)
            x_min, x_max = np.min(cc), np.max(cc)
            bounding_boxes.append([y_min, x_min, y_max, x_max])

        bounding_boxes = np.asarray(bounding_boxes)

        return mask, num_ids, bounding_boxes


    def image_reference(self, image_id):
        """Return the path of the image."""
        info = self.image_info[image_id]
        if info["source"] == "object":
            return info["path"]
        else:
            super(self.__class__, self).image_reference(image_id)

"""# CREAR INTANCIAS DEL DATASET

"""

# Crear una instancia del dataset
dataset_val = CustomDataset()

# Definir una sola ruta
ruta_unica = "/content/maskrcnn/images"
# Definir una lista de dos rutas
rutas_multiples = "/content/maskrcnn/images"

# Cargar datos desde una sola ruta
#dataset_val.load_custom(ruta_unica, "val4")
# O cargar datos desde dos rutas diferentes
dataset_val.load_custom(rutas_multiples, "train_merged")

# Preparar el dataset
dataset_val.prepare()

dataset_train = CustomDataset()
dataset_train.load_custom("/content/maskrcnn/images", "train_merged") #"TRAIN960"
dataset_train.prepare()

# Validation dataset
dataset_val = CustomDataset()
dataset_val.load_custom("/content/maskrcnn/images", "val4") #"VAL960"
dataset_val.prepare()

""".

# CARGAR EN MODO ENTRENAMIENTO
"""

# Create model in training mode
model = modellib.MaskRCNN(mode="training", config=config,
                          model_dir=MODEL_DIR)
#model.summary()

"""# TRANSFER LEARNING"""

# Which weights to start with?
init_with ="COCO" # imagenet, coco, or last
path_pasado= '/content/drive/MyDrive/logs/image960_train4_0004.h5'  #'/content/drive/MyDrive/logs/image960_train4_0004.h5'  #'/content/drive/MyDrive/logs/image960_train2_0002.h5' #'/content/drive/MyDrive/logs/image960_train2_0003.h5' #'/content/drive/MyDrive/logs/mask_rcnn_960_epoc4.h5' #'/content/drive/MyDrive/logs/latest_model2.h5'#'/content/drive/MyDrive/logs/latest_model.h5'


if init_with == "imagenet":
    model.load_weights(model.get_imagenet_weights(), by_name=True)
elif init_with == "coco":
    # Load weights trained on MS COCO, but skip layers that
    # are different due to the different number of classes
    # See README for instructions to download the COCO weights
    model.load_weights(COCO_MODEL_PATH, by_name=True,
                       exclude=["mrcnn_class_logits", "mrcnn_bbox_fc",
                                "mrcnn_bbox", "mrcnn_mask"])
elif init_with == "last":
    # Load the last model you trained and continue training
    model.load_weights(path_pasado, by_name=True)#model.find_last()

"""# VISUALIZACION DEL DATASET"""

# Load and display random samples
dataset = dataset_train
image_ids = np.random.choice(dataset.image_ids, 4)
for image_id in image_ids:
    print(image_id)
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    print(dataset.image_reference(image_id))
    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)

"""# VISUALIZACION DE LA MASCARA"""

from mrcnn.model import log
# Load random image and mask.
image_id = random.choice(dataset.image_ids)
image = dataset.load_image(image_id)
mask, class_ids = dataset.load_mask(image_id)
# Compute Bounding box
bbox = utils.extract_bboxes(mask)

# Display image and additional stats
print("image_id ", image_id, dataset.image_reference(image_id))
log("image", image)
log("mask", mask)
log("class_ids", class_ids)
log("bbox", bbox)
# Display image and instances
visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)

"""# CALLBACKS"""

#class CustomCallback(Callback):
#    def on_epoch_end(self, epoch, logs=None):
#        # Puedes personalizar esta función según tus necesidades
#        print(f"Epoch {epoch + 1} - Loss: {logs['loss']} - Val Loss: {logs['val_loss']}")

from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard

# Directorio para guardar los checkpoints y registros de TensorBoard
checkpoint_dir = '/content/checkpoints'
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

# Callback para guardar el mejor modelo durante el entrenamiento
model_checkpoint = ModelCheckpoint(
    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=True,
    mode='min',
    verbose=1
)
# Crear callbacks incorporados
early_stopping = EarlyStopping(patience=9, monitor='val_loss', restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(factor=0.0002, patience=3, monitor='val_loss')



"""# ENTRENAR ..."""

# Commented out IPython magic to ensure Python compatibility.
DRIVE_LOGS_DIR = '/content/drive/MyDrive/logs'  # GUARDAR EL ULTIMO MODELO
epocas=6
model.train(dataset_train, dataset_val,
            learning_rate=0.0003,
            epochs=epocas,
            layers='heads',
            custom_callbacks=[early_stopping, reduce_lr]
            )
# Mover el último modelo a Google Drive
latest_model_path = model.find_last()#os.path.join(MODEL_DIR, sorted(os.listdir(MODEL_DIR))[-1])
target_drive_path = os.path.join(DRIVE_LOGS_DIR, 'latest_model7.h5')

# Copiar el último modelo a Google Drive
!cp $latest_model_path $target_drive_path

frecuencia_copia = 4
# %load_ext tensorboard
# %tensorboard --logdir logs
#for epoch in range(1, epocas+ 1):  # Asegúrate de que la iteración incluya la última época
#    if epoch % frecuencia_copia == 0:
#        epoch_model_path = os.path.join(MODEL_DIR, f'model_{epoch:03d}.h5')
#        target_drive_path = os.path.join(DRIVE_LOGS_DIR, f'epoch_{epoch}')
        # Copiar el modelo de la época actual a Google Drive
#        !cp $epoch_model_path $target_drive_path

"""# HERRAMIENTAS DE VISUALIZACION DE ENTRENAMIENTO"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

"""# GUARDAR MODELO EN UNA CAPETA EN DRIVE"""

DRIVE_LOGS_DIR = '/content/drive/MyDrive/logs'
latest_model_path ='/content/maskrcnn/logs/object20240405T0556/mask_rcnn_object_0005.h5' #model.find_last()#os.path.join(MODEL_DIR, sorted(os.listdir(MODEL_DIR))[-1])
target_drive_path = os.path.join(DRIVE_LOGS_DIR, 'image960_train5_0005.h5')

# Copiar el último modelo a Google Drive
!cp $latest_model_path $target_drive_path

"""# PRUEBAS CON EL MODELO ENTRENADO EN MODO INFERENCIA"""

class InferenceConfig(CustomConfig):
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    #IMAGE_MIN_DIM = 512
    #IMAGE_MAX_DIM = 512
    DETECTION_MIN_CONFIDENCE = 0.95
inference_config = InferenceConfig()

# Recreate the model in inference mode
model = modellib.MaskRCNN(mode="inference", config=inference_config,  model_dir=MODEL_DIR)

"""# UBICACION DEL MODELO DEL CUAL SE DESEE CARGAR PESOS"""

model_path='/content/drive/MyDrive/logs/image960_train5_0005.h5'
model.load_weights(model_path, by_name=True)

"""# PROBAR EL MODELO CON CARPETA DE IMAGENES EN DRIVE"""

import skimage
import matplotlib.pyplot as plt

real_test_dir = '/content/drive/MyDrive/DATASET_960/val'
image_paths = []
for filename in os.listdir(real_test_dir):
    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:
        image_paths.append(os.path.join(real_test_dir, filename))

for image_path in image_paths:
    img = skimage.io.imread(image_path)

    if img.ndim != 3:
      image = skimage.color.gray2rgb(img)
    elif img.shape[-1] == 4:
      image = img[..., :3]
    else:
      image = img

    print(image_path)
    img_arr = np.array(image)
    results = model.detect([img_arr], verbose=1)
    r = results[0]
    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'],
                                dataset_val.class_names, r['scores'], figsize=(5,5))
    # Ajusta el tamaño de las etiquetas de clase después de la visualización
for class_id, score, bbox in zip(r['class_ids'], r['scores'], r['rois']):
    y1, x1, y2, x2 = bbox
    label = dataset_val.class_names[class_id]
    plt.annotate(f"{label} {score:.2f}", (x1, y1), color='white', backgroundcolor='red', fontsize=8)

# Muestra la imagen con las etiquetas de clase ajustadas
plt.show()

import skimage
import matplotlib.pyplot as plt

real_test_dir = '/content/drive/MyDrive/fotos_inferencia'
image_paths = []
for filename in os.listdir(real_test_dir):
    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:
        image_paths.append(os.path.join(real_test_dir, filename))

for image_path in image_paths:
    img = skimage.io.imread(image_path)
    #img = filtrar_y_reemplazar_blanco2(img)

    if img.ndim != 3:
        image = skimage.color.gray2rgb(img)
    elif img.shape[-1] == 4:
        image = img[..., :3]
    else:
        image = img

    print(image_path)
    img_arr = np.array(image)
    results = model.detect([img_arr], verbose=1)
    r = results[0]

    fig, ax = plt.subplots(figsize=(8, 6))
    ax.imshow(image)

    # Diccionario para asignar un color único a cada clase
    color_dict = {
        1: 'red',
        2: 'green',
        3: 'blue',
        4: 'orange',  # Añade más colores según sea necesario
        # Agrega más clases y colores según sea necesario
    }

    # Mostrar cuadros delimitadores y etiquetas de todas las clases detectadas
    for class_id, score, bbox in zip(r['class_ids'], r['scores'], r['rois']):
        if class_id in color_dict:  # Verifica si la clase tiene un color asignado
            y1, x1, y2, x2 = bbox
            label = dataset_val.class_names[class_id]
            color = color_dict[class_id]
            ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=color, linewidth=2))
            ax.text(x1, y1, f"{label} {score:.2f}", color='white', fontsize=8, bbox=dict(facecolor=color, alpha=0.5))

    plt.axis('off')
    plt.show()

"""# MASCARAS FILTRADAS PARA HALLAR AREA DE AFECTACION"""

import skimage
import matplotlib.pyplot as plt
conteo=0
conteo2=0
# CLASES= ['BG', 'HOJAS', 'ROYA', 'COCO', 'MINADOR']

real_test_dir = '/content/drive/MyDrive/DATASET_960/train'#'/content/maskrcnn/images/val2/'
image_paths = []
for filename in os.listdir(real_test_dir):
    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:
        image_paths.append(os.path.join(real_test_dir, filename))

for image_path in image_paths:
    img = skimage.io.imread(image_path)
    #img=filtrar_y_reemplazar_blanco2(img)

    if img.ndim != 3:
      image = skimage.color.gray2rgb(img)
    elif img.shape[-1] == 4:
      image = img[..., :3]
    else:
      image = img

    print(image_path)
    img_arr = np.array(image)
    results = model.detect([img_arr], verbose=1)
    r = results[0]
    # Encuentra la máscara correspondiente a la región más grande de la clase 1 (si existe)
    largest_mask_index = None
    largest_mask_area = 0
    for i in range(r['masks'].shape[2]):
        if r['class_ids'][i] == 1:  # Verifica si la clase es la clase 1
            mask_area = np.sum(r['masks'][:,:,i])
            if mask_area > largest_mask_area:
                largest_mask_area = mask_area
                largest_mask_index = i

    # Muestra solo la máscara correspondiente a la región más grande de la clase 1
    if largest_mask_index is not None:
        largest_mask = r['masks'][:,:,largest_mask_index]

        # Calcula el área de la región más grande de la clase 1
        area_largest_mask = np.sum(largest_mask)

        # Filtra las máscaras de las clases 2, 3 y 4
        acumulate_2 = np.zeros_like(largest_mask, dtype=np.uint8)  # Inicializa la máscara acumulada para clase 2
        acumulate_3 = np.zeros_like(largest_mask, dtype=np.uint8)  # Inicializa la máscara acumulada para clase 3
        acumulate_4 = np.zeros_like(largest_mask, dtype=np.uint8)  # Inicializa la máscara acumulada para clase 4
        for i in range(r['masks'].shape[2]):
            if r['class_ids'][i] == 2:  # Verifica si la clase es la clase 2
                acumulate_2 = np.where(r['masks'][:,:,i], 1, acumulate_2)  # Asigna valor 1 en la máscara acumulada de clase 2
            elif r['class_ids'][i] == 3:  # Verifica si la clase es la clase 3
                acumulate_3 = np.where(r['masks'][:,:,i], 1, acumulate_3)  # Asigna valor 1 en la máscara acumulada de clase 3
            elif r['class_ids'][i] == 4:  # Verifica si la clase es la clase 4
                acumulate_4 = np.where(r['masks'][:,:,i], 1, acumulate_4)  # Asigna valor 1 en la máscara acumulada de clase 4

        # Calcula la intersección entre la máscara más grande de la clase 2 y las máscaras filtradas
        acumulate_2 = np.logical_and(largest_mask, acumulate_2)
        # Calcula la intersección entre la máscara más grande de la clase 3 y las máscaras filtradas
        acumulate_3 = np.logical_and(largest_mask, acumulate_3)
        # Calcula la intersección entre la máscara más grande de la clase 4 y las máscaras filtradas
        acumulate_4 = np.logical_and(largest_mask, acumulate_4)


        # Superpone las máscaras acumuladas con colores diferentes
        overlay = np.zeros(largest_mask.shape + (3,), dtype=np.uint8)
        overlay[acumulate_2 == 1] = [255, 0, 0]  # Rojo para clase 2
        overlay[acumulate_3 == 1] = [0, 255, 0]  # Verde para clase 3
        overlay[acumulate_4 == 1] = [0, 0, 255]  # Azul para clase 4



        # Calcula el área de cada acumulación
        area_acumulate_2 = np.sum(acumulate_2)
        area_acumulate_3 = np.sum(acumulate_3)
        area_acumulate_4 = np.sum(acumulate_4)

        # Calcula la relación de área y expresa como porcentaje
        ratio_2 = (area_acumulate_2 / area_largest_mask) * 100
        ratio_3 = (area_acumulate_3 / area_largest_mask) * 100
        ratio_4 = (area_acumulate_4 / area_largest_mask) * 100

        # Imprime los resultados
        print(f"Relación de área para clase 2: {ratio_2:.2f}%")
        print(f"Relación de área para clase 3: {ratio_3:.2f}%")
        print(f"Relación de área para clase 4: {ratio_4:.2f}%")

        conteo2 += 1


        # Muestra la imagen superpuesta
        plt.imshow(largest_mask, cmap='gray')
        plt.imshow(overlay, alpha=0.5)  # Superpone las máscaras acumuladas con transparencia
        plt.title('Superposición de máscaras acumuladas')
        plt.axis('off')
        plt.show()
    else:
        print("No se encontró ninguna región de la clase 1.")
        conteo += 1

"""## FILTRO DE LUZ"""

import cv2
import numpy as np
import os  # Agregar esta línea para importar el módulo os
from shapely.geometry import Polygon , Point
def filtrar_y_reemplazar_blanco2(imagen):
    # Convertir la imagen a escala de grises
    gris = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)


    # Ajustar el umbral para separar los píxeles blancos del fondo
    _, umbral = cv2.threshold(gris, 190, 255, cv2.THRESH_BINARY)


    # Encontrar contornos en la imagen
    contours, _ = cv2.findContours(umbral, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)


    # Verificar si se detectaron contornos
    if contours:
        # Encontrar el contorno principal (el más grande)
        contorno_principal = max(contours, key=cv2.contourArea)


        # Calcular el rectángulo delimitador para el contorno principal
        x, y, w, h = cv2.boundingRect(contorno_principal)


        # Extraer la región de interés (ROI) de la imagen
        roi = imagen[y:y+h, x:x+w]


        # Calcular el polígono original
        poligono_original = Polygon(contorno_principal[:, 0, :])


        # Calcular el polígono contenedor alrededor del contorno principal con un margen de 2 píxeles
        contenedor = poligono_original.buffer(1)


        # Calcular el valor promedio de color de la región del contorno principal
        promedio_color_contorno = np.mean([roi[int(punto[0][1] - y), int(punto[0][0] - x)] for punto in contorno_principal], axis=0).astype(np.uint8)


        # Crear una máscara de la región blanca
        mascara = cv2.bitwise_not(umbral)


        # Crear una imagen completamente gris del mismo tamaño que la original
        gris = np.full_like(imagen, promedio_color_contorno, dtype=np.uint8)


        # Combinar la imagen original con el color promedio utilizando la máscara
        imagen_reemplazada = np.where(mascara[..., None], imagen, gris)
    else:
        # Si no se detectaron contornos, devolver la imagen original sin ningún procesamiento
        imagen_reemplazada = imagen


    return imagen_reemplazada

"""# IoUTest"""

import matplotlib.pyplot as plt
def IoU_function(mask_real, mask_results,plot_results=True):

    annotation_mask = np.zeros((mask_real.shape[0], mask.shape[1]))
    for i in range(mask_real.shape[2]):
        annotation_mask = np.logical_or(annotation_mask, mask_real[:,:,i])
        #plt.imshow(annotation_mask,cmap='binary')
        #plt.show()

    result_mask = np.zeros((mask_results.shape[0], mask_results.shape[1]))
    for i in range(r['masks'].shape[2]):
        result_mask = np.logical_or(result_mask, mask_results[:,:,i])
        #plt.imshow(result_mask, cmap='binary')
        #plt.show()

    intersection = np.logical_and(annotation_mask, result_mask)
    union = np.logical_or(annotation_mask, result_mask)

    IoU = np.sum(intersection) / np.sum(union)

    if(plot_results):
        colors = visualize.random_colors(2)
        IoU_image = visualize.apply_mask(image, intersection,(1.0, 0.0, 0.0), alpha=0.8)
        IoU_image = visualize.apply_mask(IoU_image, union, (0.0, 1.0, 0.0), alpha=0.3)
        plt.figure(figsize = (10,10))
        plt.text(20, 30, 'Union',bbox=dict(facecolor='green', alpha=0.5))
        plt.text(20, 50, 'Intersection',bbox=dict(facecolor='red', alpha=0.5))
        plt.title('IoU Image - Intersection over Union')
        plt.imshow(IoU_image)
        plt.show()

    return IoU;

# Validation dataset
#dataset = CascoDataset()
#dataset.load_cascos("dataset/casco", "test")
#dataset.prepare()
import matplotlib.pyplot as plt
#ENFERMAS": 1,"ROYA": 2,"COCO": 3,"MINADOR": 4,"SANAS
#class_names = ['BG', 'ENFERMAS', 'ROYA', 'COCO','MINADOR','SANAS']
class_names = ['BG', 'HOJAS', 'ROYA', 'COCO','MINADOR']
IoU_list = []


for image_id in dataset.image_ids:
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    #visualize.display_top_masks(image, mask, class_ids, dataset.class_names)

    results = model.detect([image], verbose=1)
    r = results[0]
    #visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'], figsize=(2,2))

    IoU = IoU_function(mask, r['masks'],plot_results=False)
    IoU_list.append(IoU)
    print('IoU:',IoU)

print('Number of Evaluations:', len(IoU_list))
print('Average IoU: ',np.mean(IoU_list))
print('Standard Deviacion: ', np.std(IoU_list))

plt.hist(IoU_list)
plt.show()

plt.boxplot(IoU_list)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

class_names = ['BG', 'ENFERMAS', 'ROYA', 'COCO','MINADOR','SANAS']
IoU_list = {class_name: [] for class_name in class_names}

def calculate_iou(gt_mask, pred_mask):
    intersection = np.logical_and(gt_mask, pred_mask)
    union = np.logical_or(gt_mask, pred_mask)
    iou = np.sum(intersection) / np.sum(union)
    return iou

for image_id in dataset.image_ids:
    image = dataset.load_image(image_id)
    gt_masks, gt_class_ids = dataset.load_mask(image_id)

    results = model.detect([image], verbose=1)
    r = results[0]

    for class_id in np.unique(np.concatenate([gt_class_ids, r['class_ids']])):
        gt_class_mask = np.sum(gt_masks[:, :, gt_class_ids == class_id], axis=-1, keepdims=True)
        pred_class_mask = np.sum(r['masks'][:, :, r['class_ids'] == class_id], axis=-1, keepdims=True)

        IoU = calculate_iou(gt_class_mask, pred_class_mask)
        IoU_list[class_names[class_id]].append(IoU)

        print(f'Image {image_id}, Class {class_names[class_id]} IoU: {IoU}')

# Calculate and print the average IoU for each class
for class_name in class_names:
    average_iou = np.mean(IoU_list[class_name])
    print(f'Average IoU for {class_name}: {average_iou}')

import matplotlib.pyplot as plt
import numpy as np

class_names = ['BG', 'ENFERMAS', 'ROYA', 'COCO','MINADOR','SANAS']
IoU_list = {class_name: [] for class_name in class_names}

def calculate_iou(gt_mask, pred_mask, class_name, image_id, plot_results=True):
    # Ensure both masks have the same shape
    if gt_mask.shape != pred_mask.shape:
        pred_mask = resize(pred_mask, gt_mask.shape, mode='constant', preserve_range=True)

    intersection = np.logical_and(gt_mask, pred_mask)
    union = np.logical_or(gt_mask, pred_mask)
    iou = np.sum(intersection) / np.sum(union)

    if plot_results:
        plt.figure(figsize=(10, 5))
        plt.subplot(1, 2, 1)
        plt.imshow(gt_mask, cmap='binary')
        plt.title(f'Ground Truth {class_name}')
        plt.subplot(1, 2, 2)
        plt.imshow(pred_mask, cmap='binary')
        plt.title(f'Predicted {class_name}')
        plt.show()

    return iou
# Resto del código...

for image_id in dataset.image_ids:
    image = dataset.load_image(image_id)
    gt_masks, gt_class_ids = dataset.load_mask(image_id)

    results = model.detect([image], verbose=1)
    r = results[0]

    for class_id in np.unique(np.concatenate([gt_class_ids, r['class_ids']])):
        gt_class_mask = np.sum(gt_masks[:, :, gt_class_ids == class_id], axis=-1, keepdims=True)
        pred_class_mask = np.sum(r['masks'][:, :, r['class_ids'] == class_id], axis=-1, keepdims=True)

        IoU = calculate_iou(gt_class_mask, pred_class_mask, class_names[class_id], image, plot_results=True)
        IoU_list[class_names[class_id]].append(IoU)

        print(f'Image {image_id}, Class {class_names[class_id]} IoU: {IoU}')

# Calculate and print the average IoU for each class
for class_name in class_names:
    average_iou = np.mean(IoU_list[class_name])
    print(f'Average IoU for {class_name}: {average_iou}')

clss = 5

print('Number of Evaluations for Class {}: {}'.format(class_names[clss], len(IoU_list[class_names[clss]])))
print('Average IoU for Class {}: {:.4f}'.format(class_names[clss], np.mean(IoU_list[class_names[clss]])))
print('Standard Deviation for Class {}: {:.4f}'.format(class_names[clss], np.std(IoU_list[class_names[clss]])))

plt.hist(IoU_list[class_names[clss]])
plt.title('IoU Histogram for Class {}'.format(class_names[clss]))
plt.show()

plt.boxplot(IoU_list[class_names[clss]])
plt.title('IoU Boxplot for Class {}'.format(class_names[clss]))
plt.show()

clss=1
print('Number of Evaluations:', IoU_list[class_names[clss]])
print('Average IoU: ',np.mean(IoU_list[class_names[clss]]))
print('Standard Deviacion: ', np.std(IoU_list[class_names[clss]]))

plt.hist(IoU_list[class_names[clss]])
plt.show()

plt.boxplot(IoU_list[class_names[clss]])
plt.show()

"""Iniciar TensorBoard en Google Colab (si es necesario):
Si estás utilizando Google Colab, puedes iniciar TensorBoard directamente desde una celda de código para visualizar las métricas. Ejecuta el siguiente código:
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

"""
En Google Colab, puedes utilizar TensorBoard para registrar métricas durante el entrenamiento. A continuación, se muestra un ejemplo que incluye la integración de TensorBoard en el código anterior para el entorno de Google Colab:

Primero, instala TensorBoard usando la siguiente celda en Google Colab:"""

!pip install tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/logs

"""Ajusta los parámetros según tus necesidades. ModelCheckpoint guardará el mejor modelo basado en la métrica de validación, EarlyStopping detendrá el entrenamiento si la métrica no mejora después de un número específico de épocas, y ReduceLROnPlateau reducirá la tasa de aprendizaje si la métrica no mejora después de un cierto número de épocas.

Asegúrate de adaptar las rutas de los directorios y las métricas según tu implementación específica de Mask R-CNN.
"""